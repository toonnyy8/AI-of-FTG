{"version":3,"sources":["../src/js/MirageNet/dddqn/model.js","../src/js/MirageNet/dddqn/index.js","js/agent.js"],"names":["tfex","tf","DDDQN","sequenceLen","stateVectorLen","embInner","layerNum","outputInner","actionNum","memorySize","updateTargetStep","minLearningRate","count","model","buildModel","summary","targetModel","setWeights","getWeights","memory","optimizer","train","adam","stateSeqNet","inputLayer","stateSeqLayer","layers","conv1d","filters","kernelSize","activation","padding","apply","batchNormalization","permute","dims","input","shape","i","value","globalAveragePooling1d","reshape","targetShape","flatten","A","advantage","lambda","func","x","sub","mean","Q","add","inputs","outputs","batchPrevS","batchA","batchR","batchNextS","tidy","Qs","mul","oneHot","predict","sum","maxQ","argMax","targets","scalar","replayNum","loadIdxes","usePrioritizedReplay","train_","replayIdxes","replayIdxes_","slice","arrayPrevS","arrayA","arrayR","arrayNextS","length","Math","floor","random","data","push","prevS","a","r","nextS","tensor3d","tensor1d","grads","computeGradients","tQandQ","targetQs","abs","arraySync","forEach","absTD","idx","p","loss","losses","huberLoss","print","gradsName","Object","keys","funcs","clipByGlobalNorm","values","applyGradients","reduce","acc","gn","learningRate","map","weight","prioritys","tensor","mem","softmax","multinomial","prioritizedReplayIdx","undefined","preState","action","reward","nextState","pop","unshift","index","dddqn","setBackend","dddqnModel","preArchive","state","expired","ready","then","channel","self","addEventListener","e","instruction","postMessage","args","archive","outputActions","div","actions","chooseByArgMax","chooseByMultinomial","chooseActionRandomValue","playerName","find","name","store","bsz","Ws","tList","w","weightsBuffer","sl","save","loadWeights","load","assign"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAqTC,aAAA,OAAA,eAAA,QAAA,aAAA,CAAA,OAAA,IAAA,QAAA,MAAA,EAAA,QAAA,WAAA,EArTD,IAAA,EAAA,EAAA,QAAA,qBACA,EAAA,QAAA,oCAoTC,SAAA,IAAA,GAAA,mBAAA,QAAA,OAAA,KAAA,IAAA,EAAA,IAAA,QAAA,OAAA,EAAA,WAAA,OAAA,GAAA,EAAA,SAAA,EAAA,GAAA,GAAA,GAAA,EAAA,WAAA,OAAA,EAAA,IAAA,EAAA,IAAA,GAAA,GAAA,EAAA,IAAA,GAAA,OAAA,EAAA,IAAA,GAAA,IAAA,EAAA,GAAA,GAAA,MAAA,EAAA,CAAA,IAAA,EAAA,OAAA,gBAAA,OAAA,yBAAA,IAAA,IAAA,KAAA,EAAA,GAAA,OAAA,UAAA,eAAA,KAAA,EAAA,GAAA,CAAA,IAAA,EAAA,EAAA,OAAA,yBAAA,EAAA,GAAA,KAAA,IAAA,EAAA,KAAA,EAAA,KAAA,OAAA,eAAA,EAAA,EAAA,GAAA,EAAA,GAAA,EAAA,IAAA,OAAA,EAAA,QAAA,EAAA,GAAA,EAAA,IAAA,EAAA,GAAA,EAAA,SAAA,EAAA,EAAA,GAAA,OAAA,EAAA,IAAA,EAAA,EAAA,IAAA,IAAA,SAAA,IAAA,MAAA,IAAA,UAAA,wDAAA,SAAA,EAAA,EAAA,GAAA,GAAA,OAAA,YAAA,OAAA,IAAA,uBAAA,OAAA,UAAA,SAAA,KAAA,GAAA,CAAA,IAAA,EAAA,GAAA,GAAA,EAAA,GAAA,EAAA,OAAA,EAAA,IAAA,IAAA,IAAA,EAAA,EAAA,EAAA,OAAA,cAAA,GAAA,EAAA,EAAA,QAAA,QAAA,EAAA,KAAA,EAAA,QAAA,GAAA,EAAA,SAAA,GAAA,GAAA,IAAA,MAAA,GAAA,GAAA,EAAA,EAAA,EAAA,QAAA,IAAA,GAAA,MAAA,EAAA,QAAA,EAAA,SAAA,QAAA,GAAA,EAAA,MAAA,GAAA,OAAA,GAAA,SAAA,EAAA,GAAA,GAAA,MAAA,QAAA,GAAA,OAAA,EAAA,SAAA,EAAA,EAAA,GAAA,KAAA,aAAA,GAAA,MAAA,IAAA,UAAA,qCAAA,SAAA,EAAA,EAAA,GAAA,IAAA,IAAA,EAAA,EAAA,EAAA,EAAA,OAAA,IAAA,CAAA,IAAA,EAAA,EAAA,GAAA,EAAA,WAAA,EAAA,aAAA,EAAA,EAAA,cAAA,EAAA,UAAA,IAAA,EAAA,UAAA,GAAA,OAAA,eAAA,EAAA,EAAA,IAAA,IAAA,SAAA,EAAA,EAAA,EAAA,GAAA,OAAA,GAAA,EAAA,EAAA,UAAA,GAAA,GAAA,EAAA,EAAA,GAAA,EAnTD,IAAMA,GAAO,EAAaC,EAAAA,cAAAA,GAEbC,EAiTZ,WAtSM,SAAA,EAAA,GATCC,IAAAA,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,GASf,EARCC,EAAAA,EAAAA,eAAAA,OAAiB,IAAA,EAAA,GAQlB,EAPCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,CAAC,GAAI,GAAI,IAOrB,EANCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,EAMZ,EALCC,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,CAAC,GAAI,IAKpB,EAJCC,EAAAA,EAAAA,UAAAA,OAAY,IAAA,EAAA,EAIb,EAHCC,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,IAGd,EAFCC,EAAAA,EAAAA,iBAAAA,OAAmB,IAAA,EAAA,IAEpB,EADCC,EAAAA,EAAAA,gBAAAA,OAAkB,IAAA,EAAA,KACnB,EAAA,EAAA,KAAA,GAGUD,KAAAA,iBAAmBA,EAEnBE,KAAAA,MAAQ,EAERJ,KAAAA,UAAYA,EAIZK,KAAAA,MAAQ,KAAKC,WAAW,CACzBX,YAAaA,EACbC,eAAgBA,EAChBC,SAAUA,EACVC,SAAUA,EACVC,YAAaA,EACbC,UAAWA,IAEVK,KAAAA,MAAME,UAENC,KAAAA,YAAc,KAAKF,WAAW,CAC/BX,YAAaA,EACbC,eAAgBA,EAChBC,SAAUA,EACVC,SAAUA,EACVC,YAAaA,EACbC,UAAWA,IAGVQ,KAAAA,YAAYC,WAAW,KAAKJ,MAAMK,cAIlCT,KAAAA,WAAaA,EACbU,KAAAA,OAAS,GAITR,KAAAA,gBAAkBA,EAClBS,KAAAA,UAAYnB,EAAGoB,MAAMC,KAAK,MA8P1C,OAAA,EAAA,EAAA,CAAA,CAAA,IAAA,aAlPK,MAAA,SAAA,GAgCO,IArCDnB,IAAAA,EAAAA,EAAAA,YACAC,EAAAA,EAAAA,eACAE,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,GAGjB,EAFME,EAAAA,EAAAA,UAAAA,OAAY,IAAA,EAAA,EAElB,EACMe,EAAc,SAACC,EAAYpB,EAAgBD,GAyBpCsB,OAxBPA,EAAgBxB,EAAGyB,OAAOC,OAAO,CAC7BC,QAASxB,EACTyB,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMR,GACTC,EAAgBxB,EAAGyB,OAAOO,mBAAmB,IAAID,MAAMP,GAEvDA,EAAgBxB,EAAGyB,OAAOQ,QAAQ,CAC9BC,KAAM,CAAC,EAAG,KACXH,MAAMP,GAETA,EAAgBxB,EAAGyB,OAAOC,OAAO,CAC7BC,QAASzB,EACT0B,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMP,GACTA,EAAgBxB,EAAGyB,OAAOO,mBAAmB,IAAID,MAAMP,GAEvDA,EAAgBxB,EAAGyB,OAAOQ,QAAQ,CAC9BC,KAAM,CAAC,EAAG,KACXH,MAAMP,IAITW,EAAQnC,EAAGmC,MAAM,CAAEC,MAAO,CAAClC,EAAaC,KAExCqB,EAAgBW,EAEXE,EAAI,EAAGA,EAAIhC,EAAUgC,IAC1Bb,EAAgBF,EAAYE,EAAerB,EAAiBI,EAAWL,GAGvEoC,IAAAA,EAAQd,EAERc,EAAQhB,EAAYgB,EAAOnC,EAAiBI,EAAWL,GAGvDoC,EAAQtC,EAAGyB,OAAOc,uBAAuB,IAAIR,MAAMO,GACnDA,EAAQtC,EAAGyB,OAAOe,QAAQ,CAAEC,YAAa,CAAC,EAAGtC,EAAiBI,KAAcwB,MAAMO,GAElFA,EAAQtC,EAAGyB,OAAOC,OAAO,CACrBC,QAAS,EACTC,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMO,GACTA,EAAQtC,EAAGyB,OAAOiB,UAAUX,MAAMO,GAGlCK,IAAAA,EAAInB,EAEJmB,EAAIrB,EAAYqB,EAAGxC,EAAiBI,EAAWL,GAG/CyC,EAAI3C,EAAGyB,OAAOc,uBAAuB,IAAIR,MAAMY,GAC/CA,EAAI3C,EAAGyB,OAAOe,QAAQ,CAAEC,YAAa,CAAC,EAAGtC,EAAiBI,KAAcwB,MAAMY,GAE9EA,EAAI3C,EAAGyB,OAAOC,OAAO,CACjBC,QAASpB,EACTqB,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMY,GACTA,EAAI3C,EAAGyB,OAAOiB,UAAUX,MAAMY,GAG9BC,IAAAA,EAAY7C,EAAK0B,OAAOoB,OAAO,CAC/BC,KAAM,SAACC,GACI/C,OAAAA,EAAGgD,IAAID,EAAG/C,EAAGiD,KAAKF,EAAG,GAAG,OAEpChB,MAAM,CAACY,IAENO,EAAIlD,EAAGyB,OAAO0B,MAAMpB,MAAM,CAACO,EAAOM,IAE/B5C,OAAAA,EAAGY,MAAM,CAAEwC,OAAQ,CAACjB,GAAQkB,QAASH,MAoKnD,CAAA,IAAA,SAjKUI,MAAAA,SAAAA,EAAYC,EAAQC,EAAQC,GAAY,IAAA,EAAA,KACpCzD,OAAAA,EAAG0D,KAAK,WACLC,IAAAA,EAAK3D,EAAG0D,KAAK,WACR1D,OAAAA,EAAG4D,IACN5D,EAAG6D,OAAON,EAAQ,EAAKhD,WACvB,EAAKK,MAAMkD,QAAQR,IACrBS,IAAI,KAkBH,MAAA,CAfU/D,EAAG0D,KAAK,WACfM,IAAAA,EAAOhE,EAAG4D,IACZ5D,EAAG6D,OACC7D,EAAGiE,OACC,EAAKrD,MAAMkD,QAAQL,GACnB,GAEJ,EAAKlD,WAET,EAAKQ,YAAY+C,QAAQL,IAC3BM,IAAI,GAECG,OADSV,EAAOL,IAAIa,EAAKJ,IAAI5D,EAAGmE,OAAO,SAIhCR,OAyI7B,CAAA,IAAA,QArI4E,MAAA,WAAA,IAAA,EAAA,KAAnES,EAAY,UAAA,OAAA,QAAA,IAAA,UAAA,GAAA,UAAA,GAAA,IAAKC,EAAY,UAAA,OAAA,QAAA,IAAA,UAAA,GAAA,UAAA,GAAA,CAAC,MAAOC,EAAuB,UAAA,OAAA,QAAA,IAAA,UAAA,IAAA,UAAA,GAC9DtE,EAAG0D,KAAK,WACAa,IAAAA,EAAS,SAACC,GACVxE,EAAG0D,KAAK,WAQC,IAPDe,IAAAA,EAAeD,EAAYE,QAE3BC,EAAa,GACbC,EAAS,GACTC,EAAS,GACTC,EAAa,GAERzC,EAAI,EAAGA,EAAI+B,EAAW/B,IAAK,EACT,MAAnBoC,EAAapC,IAAcoC,EAAapC,IAAM,EAAKnB,OAAO6D,UAC1DN,EAAapC,GAAK2C,KAAKC,MAAMD,KAAKE,SAAW,EAAKhE,OAAO6D,SAEzDI,IAAAA,EAAO,EAAKjE,OAAOuD,EAAapC,IAEpCsC,EAAWS,KAAKD,EAAKE,OACrBT,EAAOQ,KAAKD,EAAKG,GACjBT,EAAOO,KAAKD,EAAKI,GACjBT,EAAWM,KAAKD,EAAKK,OAGrBlC,IAAAA,EAAatD,EAAGyF,SAASd,GACzBpB,EAASvD,EAAG0F,SAASd,EAAQ,SAC7BpB,EAASxD,EAAG0F,SAASb,GACrBpB,EAAazD,EAAGyF,SAASX,GAEzBa,EAAQ,EAAKxE,UAAUyE,iBACvB,WACyB,IADnB,EAAA,EACmB,EAAKC,OACtBvC,EACAC,EACAC,EACAC,GALF,GACGqC,EADH,EAAA,GACanC,EADb,EAAA,GAOF3D,EAAG+F,IAAI/F,EAAGgD,IAAI8C,EAAUnC,IAAKqC,YACxBC,QAAQ,SAACC,EAAOC,GACb,EAAKjF,OAAOuD,EAAa0B,IAAMC,EAAIF,IAEvCG,IAAAA,EAAOrG,EAAGsG,OAAOC,UAAUT,EAAUnC,GAElC0C,OADPA,EAAKG,QACEH,GACR,EAAKzF,MAAMK,YAAW,IAAO0E,MAEhCc,EAAYC,OAAOC,KAAKhB,GAC5BA,EAAQ5F,EAAK6G,MAAMC,iBAAiBH,OAAOI,OAAOnB,GAAQ,KAAM,GAEhE,EAAKxE,UAAU4F,eAAeN,EAAUO,OAAO,SAACC,EAAKC,EAAIf,GAK9Cc,OAJPA,EAAIC,GAAMvB,EAAMQ,GAITc,GACR,KAEH,EAAKtG,QAEL,EAAKQ,UAAUgG,aAAgB,KAAO,KAAA,IAAA,EAAKxG,MAAS,IAAO,EAAKD,gBAEhE,EAAKK,YAAYC,WACb,EAAKD,YAAYE,aAAamG,IAAI,SAACC,EAAQlB,GAChCnG,OAAAA,EAAGmD,IACNnD,EAAG4D,IAAI,EAAKhD,MAAMK,aAAakF,GAAM,EAAK1F,kBAC1CT,EAAG4D,IAAIyD,EAAQ,EAAI,EAAK5G,yBAMlB,GAAtB,EAAKS,OAAO6D,QASRR,EARAD,EAC8BtE,EAAG0D,KAAK,WAC9B4D,IAAAA,EAAYtH,EAAGuH,OAAO,EAAKrG,OAAOkG,IAAI,SAAAI,GAAOA,OAAAA,EAAIpB,KAG9CpG,OAFPsH,EAAYtH,EAAGyH,QAAQH,GAEhBtH,EAAG0H,YAAYJ,EAAWlD,EAAW,MAAM,GAAM4B,cAG7BoB,IAAI,SAACO,EAAsBxB,GAC/C9B,OAAkB,MAAlBA,EAAU8B,IAAkCyB,MAAlBvD,EAAU8B,GAAoBwB,EAAuBtD,EAAU8B,KAG7F9B,OAkD1B,CAAA,IAAA,QA5CSwD,MAAAA,SAAAA,EAAUC,EAAQC,EAAQC,GACxB,KAAK9G,OAAO6D,QAAU,KAAKvE,YACtBU,KAAAA,OAAO+G,MAEX/G,KAAAA,OAAOgH,QAAQ,CAChB7C,MAAOwC,EACPvC,EAAGwC,EACHvC,EAAGwC,EACHvC,MAAOwC,EACP5B,EAAG,QAmCd,CAAA,IAAA,OA/BQ+B,MAAAA,SAAAA,GAIM,OAHM,MAATA,GAAiBA,GAAS,KAAKjH,OAAO6D,UACtCoD,EAAQnD,KAAKC,MAAMD,KAAKE,SAAW,KAAKhE,OAAO6D,SAE5C,KAAK7D,OAAOiH,OA2B1B,EAAA,GAtBM,SAASC,EAUb,GATClI,IAAAA,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,GASf,EARCC,EAAAA,EAAAA,eAAAA,OAAiB,IAAA,EAAA,GAQlB,EAPCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,CAAC,GAAI,GAAI,IAOrB,EANCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,EAMZ,EALCC,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,CAAC,GAAI,IAKpB,EAJCC,EAAAA,EAAAA,UAAAA,OAAY,IAAA,EAAA,EAIb,EAHCC,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,IAGd,EAFCC,EAAAA,EAAAA,iBAAAA,OAAmB,IAAA,EAAA,IAEpB,EADCC,EAAAA,EAAAA,gBAEO,OAAA,IAAIT,EAAM,CACbC,YAAAA,EACAC,eAAAA,EACAC,SAAAA,EACAC,SAAAA,EACAC,YAAAA,EACAC,UAAAA,EACAC,WAAAA,EACAC,iBAAAA,EACAC,qBAXc,IAAA,EAAA,KACnB,IAYF,QAAA,MAAA;;ACrTD,aAAA,OAAA,eAAA,QAAA,aAAA,CAAA,OAAA,IAAA,IAAA,EAAA,QAAA,WAAA,OAAA,KAAA,GAAA,QAAA,SAAA,GAAA,YAAA,GAAA,eAAA,GAAA,OAAA,eAAA,QAAA,EAAA,CAAA,YAAA,EAAA,IAAA,WAAA,OAAA,EAAA;;ACgCA,aAhCA,IAAA,EAAA,EAAA,QAAA,qBACA,EAAA,QAAA,gCACA,EAAA,QAAA,qCA8BA,SAAA,IAAA,GAAA,mBAAA,QAAA,OAAA,KAAA,IAAA,EAAA,IAAA,QAAA,OAAA,EAAA,WAAA,OAAA,GAAA,EAAA,SAAA,EAAA,GAAA,GAAA,GAAA,EAAA,WAAA,OAAA,EAAA,IAAA,EAAA,IAAA,GAAA,GAAA,EAAA,IAAA,GAAA,OAAA,EAAA,IAAA,GAAA,IAAA,EAAA,GAAA,GAAA,MAAA,EAAA,CAAA,IAAA,EAAA,OAAA,gBAAA,OAAA,yBAAA,IAAA,IAAA,KAAA,EAAA,GAAA,OAAA,UAAA,eAAA,KAAA,EAAA,GAAA,CAAA,IAAA,EAAA,EAAA,OAAA,yBAAA,EAAA,GAAA,KAAA,IAAA,EAAA,KAAA,EAAA,KAAA,OAAA,eAAA,EAAA,EAAA,GAAA,EAAA,GAAA,EAAA,IAAA,OAAA,EAAA,QAAA,EAAA,GAAA,EAAA,IAAA,EAAA,GAAA,EA7BA,IAAMX,GAAO,EAAaC,EAAAA,cAAAA,GAE1BA,EAAGqI,WAAW,SAEd,IAAI9H,EAAY,EAEZ+H,GAAa,EAAM,EAAA,OAAA,CACnBpI,YAAa,GACbC,eAAgB,GAChBE,SAAU,GACVE,UAAWA,EACXC,WAAY,KACZE,gBAAiB,KACjBD,iBAAkB,KAGlB8H,EAAa,CACF,QAAA,CACPC,MAAO,KACPV,OAAQ,KACRW,SAAS,GAEF,QAAA,CACPD,MAAO,KACPV,OAAQ,KACRW,SAAS,IAIjBzI,EAAG0I,QAAQC,KAAK,WACRC,IAAAA,EAAUC,KACdD,EAAQE,iBAAiB,UAAW,SAACC,GACjC/I,EAAG0D,KAAK,WACIqF,OAAAA,EAAE5D,KAAK6D,aACN,IAAA,OACDJ,EAAQK,YAAY,CAAED,YAAa,SACnC,MAEC,IAAA,OACGtC,GAA2C,GAA3CA,OAAOC,KAAKoC,EAAE5D,KAAK+D,KAAKC,SAASpE,OAAa,CAC1CqE,IAAAA,EAAgBd,EACf1H,MACAkD,QACG9D,EAAGuH,OACCb,OAAOI,OAAOiC,EAAE5D,KAAK+D,KAAKC,SACrB/B,IAAI,SAAA+B,GACMA,OAAAA,EAAQX,UAInCY,EAAgBpJ,EAAGyH,QAAQ2B,EAAe,IAC1CA,EAAgBpJ,EAAGqJ,IACfrJ,EAAGmD,IACCiG,EACA,EAAIA,EAAchH,MAAM,IAE5B,IAGUoE,QAEV8C,IAAAA,EAAU,GACVC,EAAiBH,EAAcnF,OAAO,GACrCzB,QAAQ,EAAE,IACVwD,YACDwD,EAAsBxJ,EAAG0H,YAAY0B,EAAe,EAAG,MAAM,GAC5D5G,QAAQ,EAAE,IACVwD,YACL+C,EAAE5D,KAAK+D,KAAKO,wBAAwBxD,QAAQ,SAACwD,EAAyBtD,GAC9DnB,KAAKE,SAAWuE,EAChBH,EAAQnD,GAAOqD,EAAoBrD,GAEnCmD,EAAQnD,GAAOoD,EAAepD,KAItCO,OAAOC,KAAK4B,GAAYtC,QAAQ,SAACyD,QAC8C9B,IAAvElB,OAAOC,KAAKoC,EAAE5D,KAAK+D,KAAKC,SAASQ,KAAK,SAAAC,GAAQA,OAAAA,IAASF,KACjB,GAAlCnB,EAAWmB,GAAYjB,SACvBH,EAAWuB,MACPtB,EAAWmB,GAAYlB,MACvBD,EAAWmB,GAAY5B,OACvBiB,EAAE5D,KAAK+D,KAAKC,QAAQO,GAAY3B,OAChCgB,EAAE5D,KAAK+D,KAAKC,QAAQO,GAAYlB,OAGxCD,EAAWmB,GAAYjB,SAAU,GAEjCF,EAAWmB,GAAYjB,SAAU,IAIzC/B,OAAOC,KAAKoC,EAAE5D,KAAK+D,KAAKC,SAASlD,QAAQ,SAACyD,EAAYvD,GAClDoC,EAAWmB,GAAYlB,MAAQO,EAAE5D,KAAK+D,KAAKC,QAAQO,GAAYlB,MAC/DD,EAAWmB,GAAY5B,OAASwB,EAAQnD,KAE5CyC,EAAQK,YAAY,CAChBD,YAAa,OACbE,KAAM,CACFC,QAASzC,OAAOC,KAAKoC,EAAE5D,KAAK+D,KAAKC,SAASnC,OAAO,SAACC,EAAK2C,EAAMzD,GAIlDc,OAHPA,EAAI2C,GAAQ,CACR9B,OAAQwB,EAAQnD,IAEbc,GACR,YAIX2B,EAAQK,YAAY,CAChBD,YAAa,OACbE,KAAM,CACFC,QAAS,MAKrB,MAEC,IAAA,QACDb,EAAWlH,MAAM2H,EAAE5D,KAAK+D,KAAKY,IAAKf,EAAE5D,KAAK+D,KAAK1E,YAAauE,EAAE5D,KAAK+D,KAAK5E,sBACvEsE,EAAQK,YAAY,CAAED,YAAa,UACnC,MAEC,IAAA,OACDhJ,EAAG0D,KAAK,WACAqG,IACAC,EADK1B,EAAW1H,MAAMK,aACX+F,OAAO,SAACC,EAAKgD,GAEjBhD,OADPA,EAAIgD,EAAEL,MAAQK,EACPhD,GACR,IACH2B,EAAQK,YAAY,CAChBD,YAAa,OACbE,KAAM,CACFgB,cAAenK,EAAKoK,GAAGC,KAAKJ,QAKxC,MAEC,IAAA,OACGK,IAAAA,EAActK,EAAKoK,GAAGG,KAAKvB,EAAE5D,KAAK+D,KAAKgB,eAC3C5B,EAAW1H,MAAMK,aAAagF,QAAQ,SAACgE,GACnCA,EAAEM,OAAOF,EAAYJ,EAAEL,SAE3BtB,EAAWvH,YAAYC,WACnBsH,EAAW1H,MAAMK,cAErB2H,EAAQK,YAAY,CAAED,YAAa","file":"agent.347b5559.js","sourceRoot":"..\\..\\cnnNLP_dddqn_webworker","sourcesContent":["import * as tf from \"@tensorflow/tfjs\"\r\nimport { registerTfex } from \"../../../lib/tfjs-extensions/src\"\r\nconst tfex = registerTfex(tf)\r\n\r\nexport class DDDQN {\r\n    constructor({\r\n        sequenceLen = 60,\r\n        stateVectorLen = 10,\r\n        embInner = [32, 32, 32],\r\n        layerNum = 8,\r\n        outputInner = [32, 32],\r\n        actionNum = 8,\r\n        memorySize = 1000,\r\n        updateTargetStep = 0.05,\r\n        minLearningRate = 1e-5\r\n    }) {\r\n\r\n        {\r\n            this.updateTargetStep = updateTargetStep\r\n\r\n            this.count = 0\r\n\r\n            this.actionNum = actionNum\r\n        }\r\n\r\n        {\r\n            this.model = this.buildModel({\r\n                sequenceLen: sequenceLen,\r\n                stateVectorLen: stateVectorLen,\r\n                embInner: embInner,\r\n                layerNum: layerNum,\r\n                outputInner: outputInner,\r\n                actionNum: actionNum\r\n            })\r\n            this.model.summary()\r\n\r\n            this.targetModel = this.buildModel({\r\n                sequenceLen: sequenceLen,\r\n                stateVectorLen: stateVectorLen,\r\n                embInner: embInner,\r\n                layerNum: layerNum,\r\n                outputInner: outputInner,\r\n                actionNum: actionNum\r\n            })\r\n\r\n            this.targetModel.setWeights(this.model.getWeights())\r\n        }\r\n\r\n        {\r\n            this.memorySize = memorySize\r\n            this.memory = []\r\n        }\r\n\r\n        {\r\n            this.minLearningRate = minLearningRate\r\n            this.optimizer = tf.train.adam(1e-3)\r\n        }\r\n\r\n    }\r\n\r\n    buildModel(\r\n        {\r\n            sequenceLen,\r\n            stateVectorLen,\r\n            layerNum = 32,\r\n            actionNum = 9\r\n        }\r\n    ) {\r\n        let stateSeqNet = (inputLayer, stateVectorLen, sequenceLen) => {\r\n            stateSeqLayer = tf.layers.conv1d({\r\n                filters: stateVectorLen,\r\n                kernelSize: [1],\r\n                activation: \"selu\",\r\n                padding: \"same\"\r\n            }).apply(inputLayer)\r\n            stateSeqLayer = tf.layers.batchNormalization({}).apply(stateSeqLayer)\r\n\r\n            stateSeqLayer = tf.layers.permute({\r\n                dims: [2, 1]\r\n            }).apply(stateSeqLayer)\r\n\r\n            stateSeqLayer = tf.layers.conv1d({\r\n                filters: sequenceLen,\r\n                kernelSize: [1],\r\n                activation: \"selu\",\r\n                padding: \"same\"\r\n            }).apply(stateSeqLayer)\r\n            stateSeqLayer = tf.layers.batchNormalization({}).apply(stateSeqLayer)\r\n\r\n            stateSeqLayer = tf.layers.permute({\r\n                dims: [2, 1]\r\n            }).apply(stateSeqLayer)\r\n\r\n            return stateSeqLayer\r\n        }\r\n        let input = tf.input({ shape: [sequenceLen, stateVectorLen] })\r\n\r\n        let stateSeqLayer = input\r\n\r\n        for (let i = 0; i < layerNum; i++) {\r\n            stateSeqLayer = stateSeqNet(stateSeqLayer, stateVectorLen + actionNum, sequenceLen)\r\n        }\r\n\r\n        let value = stateSeqLayer\r\n        {\r\n            value = stateSeqNet(value, stateVectorLen + actionNum, sequenceLen)\r\n\r\n            //用Global Average Pooling代替Fully Connected\r\n            value = tf.layers.globalAveragePooling1d({}).apply(value)\r\n            value = tf.layers.reshape({ targetShape: [1, stateVectorLen + actionNum] }).apply(value)\r\n\r\n            value = tf.layers.conv1d({\r\n                filters: 1,\r\n                kernelSize: [1],\r\n                activation: \"selu\",\r\n                padding: \"same\"\r\n            }).apply(value)\r\n            value = tf.layers.flatten().apply(value)\r\n        }\r\n\r\n        let A = stateSeqLayer\r\n        {\r\n            A = stateSeqNet(A, stateVectorLen + actionNum, sequenceLen)\r\n\r\n            //用Global Average Pooling代替Fully Connected\r\n            A = tf.layers.globalAveragePooling1d({}).apply(A)\r\n            A = tf.layers.reshape({ targetShape: [1, stateVectorLen + actionNum] }).apply(A)\r\n\r\n            A = tf.layers.conv1d({\r\n                filters: actionNum,\r\n                kernelSize: [1],\r\n                activation: \"selu\",\r\n                padding: \"same\"\r\n            }).apply(A)\r\n            A = tf.layers.flatten().apply(A)\r\n        }\r\n\r\n        let advantage = tfex.layers.lambda({\r\n            func: (x) => {\r\n                return tf.sub(x, tf.mean(x, 1, true))\r\n            }\r\n        }).apply([A])\r\n\r\n        let Q = tf.layers.add().apply([value, advantage])\r\n\r\n        return tf.model({ inputs: [input], outputs: Q })\r\n    }\r\n\r\n    tQandQ(batchPrevS, batchA, batchR, batchNextS) {\r\n        return tf.tidy(() => {\r\n            const Qs = tf.tidy(() => {\r\n                return tf.mul(\r\n                    tf.oneHot(batchA, this.actionNum),\r\n                    this.model.predict(batchPrevS)\r\n                ).sum(1)\r\n            })\r\n\r\n            const targetQs = tf.tidy(() => {\r\n                const maxQ = tf.mul(\r\n                    tf.oneHot(\r\n                        tf.argMax(\r\n                            this.model.predict(batchNextS),\r\n                            1\r\n                        ),\r\n                        this.actionNum\r\n                    ),\r\n                    this.targetModel.predict(batchNextS)\r\n                ).sum(1)\r\n                const targets = batchR.add(maxQ.mul(tf.scalar(0.99)));\r\n                return targets;\r\n            })\r\n\r\n            return [targetQs, Qs]\r\n        })\r\n    }\r\n\r\n    train(replayNum = 100, loadIdxes = [null], usePrioritizedReplay = false) {\r\n        tf.tidy(() => {\r\n            let train_ = (replayIdxes) => {\r\n                tf.tidy(() => {\r\n                    let replayIdxes_ = replayIdxes.slice()\r\n\r\n                    let arrayPrevS = []\r\n                    let arrayA = []\r\n                    let arrayR = []\r\n                    let arrayNextS = []\r\n\r\n                    for (let i = 0; i < replayNum; i++) {\r\n                        if (replayIdxes_[i] == null || replayIdxes_[i] >= this.memory.length) {\r\n                            replayIdxes_[i] = Math.floor(Math.random() * this.memory.length);\r\n                        }\r\n                        let data = this.memory[replayIdxes_[i]]\r\n                        // console.log(data)\r\n                        arrayPrevS.push(data.prevS)\r\n                        arrayA.push(data.a)\r\n                        arrayR.push(data.r)\r\n                        arrayNextS.push(data.nextS)\r\n                    }\r\n\r\n                    let batchPrevS = tf.tensor3d(arrayPrevS)\r\n                    let batchA = tf.tensor1d(arrayA, 'int32')\r\n                    let batchR = tf.tensor1d(arrayR)\r\n                    let batchNextS = tf.tensor3d(arrayNextS)\r\n\r\n                    let grads = this.optimizer.computeGradients(\r\n                        () => {\r\n                            let [targetQs, Qs] = this.tQandQ(\r\n                                batchPrevS,\r\n                                batchA,\r\n                                batchR,\r\n                                batchNextS\r\n                            )\r\n                            tf.abs(tf.sub(targetQs, Qs)).arraySync()\r\n                                .forEach((absTD, idx) => {\r\n                                    this.memory[replayIdxes_[idx]].p = absTD\r\n                                })\r\n                            let loss = tf.losses.huberLoss(targetQs, Qs)\r\n                            loss.print()\r\n                            return loss\r\n                        }, this.model.getWeights(true)).grads\r\n\r\n                    let gradsName = Object.keys(grads)\r\n                    grads = tfex.funcs.clipByGlobalNorm(Object.values(grads), 0.05)[0]\r\n\r\n                    this.optimizer.applyGradients(gradsName.reduce((acc, gn, idx) => {\r\n                        acc[gn] = grads[idx]\r\n                        // if (gn == \"weighted_average_WeightedAverage1/w\") {\r\n                        //     acc[gn].print()\r\n                        // }\r\n                        return acc\r\n                    }, {}))\r\n\r\n                    this.count++\r\n\r\n                    this.optimizer.learningRate = (1e-4 / this.count ** 0.5) + this.minLearningRate\r\n\r\n                    this.targetModel.setWeights(\r\n                        this.targetModel.getWeights().map((weight, idx) => {\r\n                            return tf.add(\r\n                                tf.mul(this.model.getWeights()[idx], this.updateTargetStep),\r\n                                tf.mul(weight, 1 - this.updateTargetStep),\r\n                            )\r\n                        })\r\n                    )\r\n                })\r\n            }\r\n            if (this.memory.length != 0) {\r\n                if (usePrioritizedReplay) {\r\n                    let prioritizedReplayBuffer = tf.tidy(() => {\r\n                        let prioritys = tf.tensor(this.memory.map(mem => mem.p))\r\n                        prioritys = tf.softmax(prioritys)\r\n                        // prioritys.print()\r\n                        return tf.multinomial(prioritys, replayNum, null, true).arraySync()\r\n                    })\r\n                    // console.log(prioritizedReplayBuffer)\r\n                    train_(prioritizedReplayBuffer.map((prioritizedReplayIdx, idx) => {\r\n                        return loadIdxes[idx] == null || loadIdxes[idx] == undefined ? prioritizedReplayIdx : loadIdxes[idx]\r\n                    }))\r\n                } else {\r\n                    train_(loadIdxes)\r\n                }\r\n            }\r\n        })\r\n    }\r\n\r\n    store(preState, action, reward, nextState) {\r\n        if (this.memory.length == this.memorySize) {\r\n            this.memory.pop()\r\n        }\r\n        this.memory.unshift({\r\n            prevS: preState,\r\n            a: action,\r\n            r: reward,\r\n            nextS: nextState,\r\n            p: 1e+9\r\n        })\r\n    }\r\n\r\n    load(index) {\r\n        if (index == null || index >= this.memory.length) {\r\n            index = Math.floor(Math.random() * this.memory.length);\r\n        }\r\n        return this.memory[index]\r\n    }\r\n\r\n}\r\n\r\nexport function dddqn({\r\n    sequenceLen = 60,\r\n    stateVectorLen = 10,\r\n    embInner = [32, 32, 32],\r\n    layerNum = 8,\r\n    outputInner = [32, 32],\r\n    actionNum = 8,\r\n    memorySize = 1000,\r\n    updateTargetStep = 0.05,\r\n    minLearningRate = 1e-3\r\n}) {\r\n    return new DDDQN({\r\n        sequenceLen,\r\n        stateVectorLen,\r\n        embInner,\r\n        layerNum,\r\n        outputInner,\r\n        actionNum,\r\n        memorySize,\r\n        updateTargetStep,\r\n        minLearningRate\r\n    })\r\n}","export * from './model'","import * as tf from \"@tensorflow/tfjs\"\r\nimport { dddqn } from \"../../src/js/MirageNet/dddqn\"\r\nimport { registerTfex } from \"../../src/lib/tfjs-extensions/src\"\r\nconst tfex = registerTfex(tf)\r\n\r\ntf.setBackend(\"webgl\")\r\n\r\nlet actionNum = 8\r\n\r\nlet dddqnModel = dddqn({\r\n    sequenceLen: 16,\r\n    stateVectorLen: 55,\r\n    layerNum: 16,\r\n    actionNum: actionNum,\r\n    memorySize: 3200,\r\n    minLearningRate: 5e-4,\r\n    updateTargetStep: 0.1\r\n})\r\n\r\nlet preArchive = {\r\n    \"player1\": {\r\n        state: null,\r\n        action: null,\r\n        expired: true\r\n    },\r\n    \"player2\": {\r\n        state: null,\r\n        action: null,\r\n        expired: true\r\n    }\r\n}\r\n\r\ntf.ready().then(() => {\r\n    let channel = self\r\n    channel.addEventListener(\"message\", (e) => {\r\n        tf.tidy(() => {\r\n            switch (e.data.instruction) {\r\n                case 'init': {\r\n                    channel.postMessage({ instruction: \"init\" })\r\n                    break\r\n                }\r\n                case 'ctrl': {\r\n                    if (Object.keys(e.data.args.archive).length != 0) {\r\n                        let outputActions = dddqnModel\r\n                            .model\r\n                            .predict(\r\n                                tf.tensor(\r\n                                    Object.values(e.data.args.archive)\r\n                                        .map(archive => {\r\n                                            return archive.state\r\n                                        })\r\n                                )\r\n                            )\r\n                        outputActions = tf.softmax(outputActions, 1)\r\n                        outputActions = tf.div(\r\n                            tf.add(\r\n                                outputActions,\r\n                                1 / outputActions.shape[1]\r\n                            ),\r\n                            2\r\n                        )\r\n                        // outputActions.sum(1).print()\r\n                        outputActions.print()\r\n\r\n                        let actions = []\r\n                        let chooseByArgMax = outputActions.argMax(1)\r\n                            .reshape([-1])\r\n                            .arraySync()\r\n                        let chooseByMultinomial = tf.multinomial(outputActions, 1, null, true)\r\n                            .reshape([-1])\r\n                            .arraySync()\r\n                        e.data.args.chooseActionRandomValue.forEach((chooseActionRandomValue, idx) => {\r\n                            if (Math.random() < chooseActionRandomValue) {\r\n                                actions[idx] = chooseByMultinomial[idx]\r\n                            } else {\r\n                                actions[idx] = chooseByArgMax[idx]\r\n                            }\r\n                        })\r\n\r\n                        Object.keys(preArchive).forEach((playerName) => {\r\n                            if (Object.keys(e.data.args.archive).find(name => name === playerName) !== undefined) {\r\n                                if (preArchive[playerName].expired == false) {\r\n                                    dddqnModel.store(\r\n                                        preArchive[playerName].state,\r\n                                        preArchive[playerName].action,\r\n                                        e.data.args.archive[playerName].reward,\r\n                                        e.data.args.archive[playerName].state,\r\n                                    )\r\n                                }\r\n                                preArchive[playerName].expired = false\r\n                            } else {\r\n                                preArchive[playerName].expired = true\r\n                            }\r\n                        })\r\n\r\n                        Object.keys(e.data.args.archive).forEach((playerName, idx) => {\r\n                            preArchive[playerName].state = e.data.args.archive[playerName].state\r\n                            preArchive[playerName].action = actions[idx]\r\n                        })\r\n                        channel.postMessage({\r\n                            instruction: \"ctrl\",\r\n                            args: {\r\n                                archive: Object.keys(e.data.args.archive).reduce((acc, name, idx) => {\r\n                                    acc[name] = {\r\n                                        action: actions[idx]\r\n                                    }\r\n                                    return acc\r\n                                }, {})\r\n                            }\r\n                        })\r\n                    } else {\r\n                        channel.postMessage({\r\n                            instruction: \"ctrl\",\r\n                            args: {\r\n                                archive: {}\r\n                            }\r\n                        })\r\n                    }\r\n                    // console.log(\"ctrl\")\r\n                    break\r\n                }\r\n                case 'train': {\r\n                    dddqnModel.train(e.data.args.bsz, e.data.args.replayIdxes, e.data.args.usePrioritizedReplay)\r\n                    channel.postMessage({ instruction: \"train\" })\r\n                    break\r\n                }\r\n                case 'save': {\r\n                    tf.tidy(() => {\r\n                        let Ws = dddqnModel.model.getWeights()\r\n                        let tList = Ws.reduce((acc, w) => {\r\n                            acc[w.name] = w\r\n                            return acc\r\n                        }, {})\r\n                        channel.postMessage({\r\n                            instruction: \"save\",\r\n                            args: {\r\n                                weightsBuffer: tfex.sl.save(tList)\r\n                            }\r\n                        })\r\n                    })\r\n\r\n                    break\r\n                }\r\n                case 'load': {\r\n                    let loadWeights = tfex.sl.load(e.data.args.weightsBuffer)\r\n                    dddqnModel.model.getWeights().forEach((w) => {\r\n                        w.assign(loadWeights[w.name])\r\n                    })\r\n                    dddqnModel.targetModel.setWeights(\r\n                        dddqnModel.model.getWeights()\r\n                    )\r\n                    channel.postMessage({ instruction: \"load\" })\r\n                    break\r\n                }\r\n            }\r\n        })\r\n    })\r\n})"]}