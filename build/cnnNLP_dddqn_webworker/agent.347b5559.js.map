{"version":3,"sources":["../src/js/MirageNet/dddqn/model.js","../src/js/MirageNet/dddqn/index.js","js/agent.js"],"names":["tfex","tf","DDDQN","sequenceLen","stateVectorLen","embInner","layerNum","outputInner","actionsNum","memorySize","updateTargetStep","initLearningRate","minLearningRate","discount","count","model","buildModel","summary","targetModel","setWeights","getWeights","memory","optimizer","train","adam","stateSeqNet","inputLayer","stateSeqLayer","layers","conv1d","filters","kernelSize","activation","padding","apply","permute","dims","input","shape","i","reduce","a","b","outputs","map","actionNum","actionSeqLayer","Math","ceil","value","flatten","A","Q","lambda","func","x","sub","mean","add","inputs","batchPrevS","batchAs","batchRs","batchNextS","tidy","predictions","predict","length","Qs","actionType","mul","oneHot","sum","targetPredictions","maxQ","argMax","targets","scalar","replayNum","loadIdxes","usePrioritizedReplay","train_","replayIdxes","replayIdxes_","slice","arrayPrevS","arrayAs","Array","fill","arrayRs","arrayNextS","floor","random","data","push","prevS","j","As","Rs","nextS","tensor3d","arrayA","tensor1d","arrayR","grads","computeGradients","tQandQ","targetQs","addN","abs","arraySync","forEach","absTD","idx","p","loss","stack","losses","huberLoss","print","gradsName","Object","keys","funcs","clipByGlobalNorm","values","applyGradients","acc","gn","learningRate","max","weight","prioritys","tensor","mem","div","multinomial","prioritizedReplayIdx","undefined","preState","actions","rewards","nextState","pop","unshift","index","dddqn","setBackend","dddqnModel","preArchives","state","expired","ready","then","channel","self","addEventListener","e","instruction","postMessage","args","archives","outputActions","archive","outputAction","softmax","chooseByArgMax","concat","reshape","transpose","chooseByMultinomial","playerName","chooseActionRandomValue","find","name","store","aiCtrl","bsz","Ws","tList","w","weightsBuffer","sl","save","loadWeights","load","assign"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6WC,aAAA,OAAA,eAAA,QAAA,aAAA,CAAA,OAAA,IAAA,QAAA,MAAA,EAAA,QAAA,WAAA,EA7WD,IAAA,EAAA,EAAA,QAAA,qBACA,EAAA,QAAA,oCA4WC,SAAA,IAAA,GAAA,mBAAA,QAAA,OAAA,KAAA,IAAA,EAAA,IAAA,QAAA,OAAA,EAAA,WAAA,OAAA,GAAA,EAAA,SAAA,EAAA,GAAA,GAAA,GAAA,EAAA,WAAA,OAAA,EAAA,IAAA,EAAA,IAAA,GAAA,GAAA,EAAA,IAAA,GAAA,OAAA,EAAA,IAAA,GAAA,IAAA,EAAA,GAAA,GAAA,MAAA,EAAA,CAAA,IAAA,EAAA,OAAA,gBAAA,OAAA,yBAAA,IAAA,IAAA,KAAA,EAAA,GAAA,OAAA,UAAA,eAAA,KAAA,EAAA,GAAA,CAAA,IAAA,EAAA,EAAA,OAAA,yBAAA,EAAA,GAAA,KAAA,IAAA,EAAA,KAAA,EAAA,KAAA,OAAA,eAAA,EAAA,EAAA,GAAA,EAAA,GAAA,EAAA,IAAA,OAAA,EAAA,QAAA,EAAA,GAAA,EAAA,IAAA,EAAA,GAAA,EAAA,SAAA,EAAA,EAAA,GAAA,OAAA,EAAA,IAAA,EAAA,EAAA,IAAA,IAAA,SAAA,IAAA,MAAA,IAAA,UAAA,wDAAA,SAAA,EAAA,EAAA,GAAA,GAAA,OAAA,YAAA,OAAA,IAAA,uBAAA,OAAA,UAAA,SAAA,KAAA,GAAA,CAAA,IAAA,EAAA,GAAA,GAAA,EAAA,GAAA,EAAA,OAAA,EAAA,IAAA,IAAA,IAAA,EAAA,EAAA,EAAA,OAAA,cAAA,GAAA,EAAA,EAAA,QAAA,QAAA,EAAA,KAAA,EAAA,QAAA,GAAA,EAAA,SAAA,GAAA,GAAA,IAAA,MAAA,GAAA,GAAA,EAAA,EAAA,EAAA,QAAA,IAAA,GAAA,MAAA,EAAA,QAAA,EAAA,SAAA,QAAA,GAAA,EAAA,MAAA,GAAA,OAAA,GAAA,SAAA,EAAA,GAAA,GAAA,MAAA,QAAA,GAAA,OAAA,EAAA,SAAA,EAAA,EAAA,GAAA,KAAA,aAAA,GAAA,MAAA,IAAA,UAAA,qCAAA,SAAA,EAAA,EAAA,GAAA,IAAA,IAAA,EAAA,EAAA,EAAA,EAAA,OAAA,IAAA,CAAA,IAAA,EAAA,EAAA,GAAA,EAAA,WAAA,EAAA,aAAA,EAAA,EAAA,cAAA,EAAA,UAAA,IAAA,EAAA,UAAA,GAAA,OAAA,eAAA,EAAA,EAAA,IAAA,IAAA,SAAA,EAAA,EAAA,EAAA,GAAA,OAAA,GAAA,EAAA,EAAA,UAAA,GAAA,GAAA,EAAA,EAAA,GAAA,EA3WD,IAAMA,GAAO,EAAaC,EAAAA,cAAAA,GAEbC,EAyWZ,WA5VM,SAAA,EAAA,GAXCC,IAAAA,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,GAWf,EAVCC,EAAAA,EAAAA,eAAAA,OAAiB,IAAA,EAAA,GAUlB,EATCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,CAAC,GAAI,GAAI,IASrB,EARCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,EAQZ,EAPCC,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,CAAC,GAAI,IAOpB,EANCC,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,CAAC,EAAG,EAAG,GAMrB,EALCC,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,IAKd,EAJCC,EAAAA,EAAAA,iBAAAA,OAAmB,IAAA,EAAA,IAIpB,EAHCC,EAAAA,EAAAA,iBAAAA,OAAmB,IAAA,EAAA,KAGpB,EAFCC,EAAAA,EAAAA,gBAAAA,OAAkB,IAAA,EAAA,KAEnB,EADCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,IACZ,EAAA,EAAA,KAAA,GAGUH,KAAAA,iBAAmBA,EAEnBG,KAAAA,SAAWA,EAEXC,KAAAA,MAAQ,EAERN,KAAAA,WAAaA,EAIbO,KAAAA,MAAQ,KAAKC,WAAW,CACzBb,YAAaA,EACbC,eAAgBA,EAChBC,SAAUA,EACVC,SAAUA,EACVC,YAAaA,EACbC,WAAYA,IAEXO,KAAAA,MAAME,UAENC,KAAAA,YAAc,KAAKF,WAAW,CAC/Bb,YAAaA,EACbC,eAAgBA,EAChBC,SAAUA,EACVC,SAAUA,EACVC,YAAaA,EACbC,WAAYA,IAGXU,KAAAA,YAAYC,WAAW,KAAKJ,MAAMK,cAIlCX,KAAAA,WAAaA,EACbY,KAAAA,OAAS,GAITT,KAAAA,gBAAkBA,EAClBD,KAAAA,iBAAmBA,EACnBW,KAAAA,UAAYrB,EAAGsB,MAAMC,KAAK,KAAKb,kBAiT/C,OAAA,EAAA,EAAA,CAAA,CAAA,IAAA,aArSK,MAAA,SAAA,GA8BO,IAnCDR,IAAAA,EAAAA,EAAAA,YACAC,EAAAA,EAAAA,eACAE,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,GAGjB,EAFME,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,CAAC,EAAG,EAAG,GAE1B,EACMiB,EAAc,SAACC,EAAYtB,EAAgBD,GAuBpCwB,OAtBPA,EAAgB1B,EAAG2B,OAAOC,OAAO,CAC7BC,QAAS1B,EACT2B,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMR,GAETC,EAAgB1B,EAAG2B,OAAOO,QAAQ,CAC9BC,KAAM,CAAC,EAAG,KACXF,MAAMP,GAETA,EAAgB1B,EAAG2B,OAAOC,OAAO,CAC7BC,QAAS3B,EACT4B,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMP,GAETA,EAAgB1B,EAAG2B,OAAOO,QAAQ,CAC9BC,KAAM,CAAC,EAAG,KACXF,MAAMP,IAITU,EAAQpC,EAAGoC,MAAM,CAAEC,MAAO,CAACnC,EAAaC,KAExCuB,EAAgBF,EAAYY,EAAwB,EAAjBjC,EAAoBD,GAElDoC,EAAI,EAAGA,EAAIjC,EAAUiC,IAC1BZ,EAAgBF,EAAYE,EAAuD,EAAxCnB,EAAWgC,OAAO,SAACC,EAAGC,GAAMD,OAAAA,EAAIC,GAAG,GAAQvC,GAGtFwC,IAAAA,EAAUnC,EAAWoC,IAAI,SAAAC,GAGpB,IAFDC,IAAAA,EAAiBrB,EAAYE,EAAuD,EAAxCnB,EAAWgC,OAAO,SAACC,EAAGC,GAAMD,OAAAA,EAAIC,GAAG,GAAQvC,GAElFoC,EAAI,EAAGA,EAAIQ,KAAKC,KAAK1C,KAAAA,IAAAA,EAAY,KAAMiC,IAC5CO,EAAiBrB,EAAYqB,EAAgBtC,EAAWgC,OAAO,SAACC,EAAGC,GAAMD,OAAAA,EAAIC,GAAG,GAAIvC,GAQpF8C,IAAAA,EALJH,EAAiB7C,EAAG2B,OAAOO,QAAQ,CAC/BC,KAAM,CAAC,EAAG,KACXF,MAAMY,GAKLG,EAAQhD,EAAG2B,OAAOC,OAAO,CACrBC,QAAS,EACTC,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMe,GAETA,EAAQhD,EAAG2B,OAAOO,QAAQ,CACtBC,KAAM,CAAC,EAAG,KACXF,MAAMe,GAETA,EAAQhD,EAAG2B,OAAOC,OAAO,CACrBC,QAAS,EACTC,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMe,GACTA,EAAQhD,EAAG2B,OAAOsB,UAAUhB,MAAMe,GAGlCE,IAAAA,EAAIL,EA8BDM,OA5BHD,EAAIlD,EAAG2B,OAAOC,OAAO,CACjBC,QAAS,EACTC,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMiB,GAETA,EAAIlD,EAAG2B,OAAOO,QAAQ,CAClBC,KAAM,CAAC,EAAG,KACXF,MAAMiB,GAETA,EAAIlD,EAAG2B,OAAOC,OAAO,CACjBC,QAASe,EACTd,WAAY,CAAC,GACbC,WAAY,OACZC,QAAS,SACVC,MAAMiB,GACTA,EAAIlD,EAAG2B,OAAOsB,UAAUhB,MAAMiB,GAE9BA,EAAInD,EAAK4B,OAAOyB,OAAO,CACnBC,KAAM,SAACC,GACItD,OAAAA,EAAGuD,IAAID,EAAGtD,EAAGwD,KAAKF,EAAG,GAAG,OAEpCrB,MAAM,CAACiB,IAGNlD,EAAG2B,OAAO8B,MAAMxB,MAAM,CAACe,EAAOE,MAKnClD,OAAAA,EAAGc,MAAM,CAAE4C,OAAQ,CAACtB,GAAQM,QAASA,MAgMnD,CAAA,IAAA,SA7LUiB,MAAAA,SAAAA,EAAYC,EAASC,EAASC,GAAY,IAAA,EAAA,KACtC9D,OAAAA,EAAG+D,KAAK,WACPC,IAAAA,EAAc,EAAKlD,MAAMmD,QAAQN,GACP,GAA1B,EAAKpD,WAAW2D,SAChBF,EAAc,CAACA,IAEbG,IAAAA,EAAK,EAAK5D,WAAWoC,IAAI,SAACC,EAAWwB,GAChCpE,OAAAA,EAAGqE,IACNrE,EAAGsE,OACCV,EAAQQ,GACRxB,GAEJoB,EAAYI,IACdG,IAAI,KAGNC,EAAoB,EAAKvD,YAAYgD,QAAQH,GAkB1C,OAjBuB,GAA1B,EAAKvD,WAAW2D,SAChBM,EAAoB,CAACA,IAgBlB,CAdU,EAAKjE,WAAWoC,IAAI,SAACC,EAAWwB,GACvCK,IAAAA,EAAOzE,EAAGqE,IACZrE,EAAGsE,OACCtE,EAAG0E,OACCV,EAAYI,GACZ,GAEJxB,GAEJ4B,EAAkBJ,IACpBG,IAAI,GAECI,OADSd,EAAQO,GAAYX,IAAIgB,EAAKJ,IAAIrE,EAAG4E,OAAO,EAAKhE,cAGlDuD,OA2J7B,CAAA,IAAA,QAvJ4E,MAAA,WAAA,IAAA,EAAA,KAAnEU,EAAY,UAAA,OAAA,QAAA,IAAA,UAAA,GAAA,UAAA,GAAA,IAAKC,EAAY,UAAA,OAAA,QAAA,IAAA,UAAA,GAAA,UAAA,GAAA,CAAC,MAAOC,EAAuB,UAAA,OAAA,QAAA,IAAA,UAAA,IAAA,UAAA,GAC9D/E,EAAG+D,KAAK,WACAiB,IAAAA,EAAS,SAACC,GACVjF,EAAG+D,KAAK,WAQC,IAPDmB,IAAAA,EAAeD,EAAYE,QAE3BC,EAAa,GACbC,EAAU,IAAIC,MAAM,EAAK/E,WAAW2D,QAAQqB,KAAK,IACjDC,EAAU,IAAIF,MAAM,EAAK/E,WAAW2D,QAAQqB,KAAK,IACjDE,EAAa,GAERnD,EAAI,EAAGA,EAAIuC,EAAWvC,IAAK,EACT,MAAnB4C,EAAa5C,IAAc4C,EAAa5C,IAAM,EAAKlB,OAAO8C,UAC1DgB,EAAa5C,GAAKQ,KAAK4C,MAAM5C,KAAK6C,SAAW,EAAKvE,OAAO8C,SAEzD0B,IAAAA,EAAO,EAAKxE,OAAO8D,EAAa5C,IAEpC8C,EAAWS,KAAKD,EAAKE,OAChB,IAAA,IAAIC,EAAI,EAAGA,EAAI,EAAKxF,WAAW2D,OAAQ6B,IACxCV,EAAQU,GAAGzD,GAAKsD,EAAKI,GAAGD,GACxBP,EAAQO,GAAGzD,GAAKsD,EAAKK,GAAGF,GAE5BN,EAAWI,KAAKD,EAAKM,OAGrBvC,IAAAA,EAAa3D,EAAGmG,SAASf,GACzBxB,EAAUyB,EAAQ1C,IAAI,SAACyD,GAChBpG,OAAAA,EAAGqG,SAASD,EAAQ,WAE3BvC,EAAU2B,EAAQ7C,IAAI,SAAC2D,GAChBtG,OAAAA,EAAGqG,SAASC,EAAQ,WAE3BxC,EAAa9D,EAAGmG,SAASV,GAEzBc,EAAQ,EAAKlF,UAAUmF,iBACvB,WACyB,IADnB,EAAA,EACmB,EAAKC,OACtB9C,EACAC,EACAC,EACAC,GALF,GACG4C,EADH,EAAA,GACavC,EADb,EAAA,GAOFnE,EAAG2G,KACC,EAAKpG,WAAWoC,IAAI,SAACC,EAAWwB,GACrBpE,OAAAA,EAAG4G,IAAI5G,EAAGuD,IAAImD,EAAStC,GAAaD,EAAGC,QAEpDyC,YACGC,QAAQ,SAACC,EAAOC,GACb,EAAK5F,OAAO8D,EAAa8B,IAAMC,EAAIF,IAEvCG,IAAAA,EAAOlH,EAAGwD,KACVxD,EAAGmH,MACC,EAAK5G,WAAWoC,IAAI,SAACC,EAAWwB,GACrBpE,OAAAA,EAAGoH,OAAOC,UAAUX,EAAStC,GAAaD,EAAGC,QAKzD8C,OADPA,EAAKI,QACEJ,GACR,EAAKpG,MAAMK,YAAW,IAAOoF,MAEhCgB,EAAYC,OAAOC,KAAKlB,GAC5BA,EAAQxG,EAAK2H,MAAMC,iBAAiBH,OAAOI,OAAOrB,GAAQ,KAAM,GAEhE,EAAKlF,UAAUwG,eAAeN,EAAUhF,OAAO,SAACuF,EAAKC,EAAIf,GAK9Cc,OAJPA,EAAIC,GAAMxB,EAAMS,GAITc,GACR,KAEH,EAAKjH,QAEL,EAAKQ,UAAU2G,aAAelF,KAAKmF,IAAI,EAAKvH,iBAAoB,KAAA,IAAA,EAAKG,MAAS,IAAM,EAAKF,iBAEzF,EAAKM,YAAYC,WACb,EAAKD,YAAYE,aAAawB,IAAI,SAACuF,EAAQlB,GAChChH,OAAAA,EAAGyD,IACNzD,EAAGqE,IAAI,EAAKvD,MAAMK,aAAa6F,GAAM,EAAKvG,kBAC1CT,EAAGqE,IAAI6D,EAAQ,EAAI,EAAKzH,yBAMlB,GAAtB,EAAKW,OAAO8C,QASRc,EARAD,EAC8B/E,EAAG+D,KAAK,WAC9BoE,IAAAA,EAAYnI,EAAGoI,OAAO,EAAKhH,OAAOuB,IAAI,SAAA0F,GAAOA,OAAAA,EAAIpB,KAG9CjH,OAFPmI,EAAYnI,EAAGsI,IAAIH,EAAWnI,EAAGuE,IAAI4D,EAAW,GAAG,IAE5CnI,EAAGuI,YAAYJ,EAAWtD,EAAW,MAAM,GAAMgC,cAG7BlE,IAAI,SAAC6F,EAAsBxB,GAC/ClC,OAAkB,MAAlBA,EAAUkC,IAAkCyB,MAAlB3D,EAAUkC,GAAoBwB,EAAuB1D,EAAUkC,KAG7FlC,OAoD1B,CAAA,IAAA,QA9CS4D,MAAAA,SAAAA,EAAUC,EAASC,EAASC,GAC1B,KAAKzH,OAAO8C,QAAU,KAAK1D,YACtBY,KAAAA,OAAO0H,MAEX1H,KAAAA,OAAO2H,QAAQ,CAChBjD,MAAO4C,EACP1C,GAAI2C,EACJ1C,GAAI2C,EACJ1C,MAAO2C,EACP5B,EAAG,QAqCd,CAAA,IAAA,OAjCQ+B,MAAAA,SAAAA,GAIM,OAHM,MAATA,GAAiBA,GAAS,KAAK5H,OAAO8C,UACtC8E,EAAQlG,KAAK4C,MAAM5C,KAAK6C,SAAW,KAAKvE,OAAO8C,SAE5C,KAAK9C,OAAO4H,OA6B1B,EAAA,GAxBM,SAASC,EAWb,GAVC/I,IAAAA,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,GAUf,EATCC,EAAAA,EAAAA,eAAAA,OAAiB,IAAA,EAAA,GASlB,EARCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,CAAC,GAAI,GAAI,IAQrB,EAPCC,EAAAA,EAAAA,SAAAA,OAAW,IAAA,EAAA,EAOZ,EANCC,EAAAA,EAAAA,YAAAA,OAAc,IAAA,EAAA,CAAC,GAAI,IAMpB,EALCC,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,CAAC,EAAG,EAAG,GAKrB,EAJCC,EAAAA,EAAAA,WAAAA,OAAa,IAAA,EAAA,IAId,EAHCC,EAAAA,EAAAA,iBAAAA,OAAmB,IAAA,EAAA,IAGpB,EAFCE,EAAAA,EAAAA,gBAAAA,OAAkB,IAAA,EAAA,KAEnB,EADCC,EAAAA,EAAAA,SAEO,OAAA,IAAIX,EAAM,CACbC,YAAAA,EACAC,eAAAA,EACAC,SAAAA,EACAC,SAAAA,EACAC,YAAAA,EACAC,WAAAA,EACAC,WAAAA,EACAC,iBAAAA,EACAE,gBAAAA,EACAC,cAZO,IAAA,EAAA,IACZ,IAaF,QAAA,MAAA;;AC7WD,aAAA,OAAA,eAAA,QAAA,aAAA,CAAA,OAAA,IAAA,IAAA,EAAA,QAAA,WAAA,OAAA,KAAA,GAAA,QAAA,SAAA,GAAA,YAAA,GAAA,eAAA,GAAA,OAAA,eAAA,QAAA,EAAA,CAAA,YAAA,EAAA,IAAA,WAAA,OAAA,EAAA;;ACgCA,aAhCA,IAAA,EAAA,EAAA,QAAA,qBACA,EAAA,QAAA,gCACA,EAAA,QAAA,qCA8BA,SAAA,IAAA,GAAA,mBAAA,QAAA,OAAA,KAAA,IAAA,EAAA,IAAA,QAAA,OAAA,EAAA,WAAA,OAAA,GAAA,EAAA,SAAA,EAAA,GAAA,GAAA,GAAA,EAAA,WAAA,OAAA,EAAA,IAAA,EAAA,IAAA,GAAA,GAAA,EAAA,IAAA,GAAA,OAAA,EAAA,IAAA,GAAA,IAAA,EAAA,GAAA,GAAA,MAAA,EAAA,CAAA,IAAA,EAAA,OAAA,gBAAA,OAAA,yBAAA,IAAA,IAAA,KAAA,EAAA,GAAA,OAAA,UAAA,eAAA,KAAA,EAAA,GAAA,CAAA,IAAA,EAAA,EAAA,OAAA,yBAAA,EAAA,GAAA,KAAA,IAAA,EAAA,KAAA,EAAA,KAAA,OAAA,eAAA,EAAA,EAAA,GAAA,EAAA,GAAA,EAAA,IAAA,OAAA,EAAA,QAAA,EAAA,GAAA,EAAA,IAAA,EAAA,GAAA,EA7BA,IAAMb,GAAO,EAAaC,EAAAA,cAAAA,GAE1BA,EAAGkJ,WAAW,SAEd,IAAI3I,EAAa,CAAC,EAAG,EAAG,EAAG,EAAG,EAAG,EAAG,GAEhC4I,GAAa,EAAM,EAAA,OAAA,CACnBjJ,YAAa,GACbC,eAAgB,GAChBE,SAAU,GACVE,WAAYA,EACZC,WAAY,KACZG,gBAAiB,KACjBD,iBAAkB,KAClBD,iBAAkB,GAClBG,SAAU,KAGVwI,EAAc,CACH,QAAA,CACPC,MAAO,KACPC,SAAS,GAEF,QAAA,CACPD,MAAO,KACPC,SAAS,IAIjBtJ,EAAGuJ,QAAQC,KAAK,WACRC,IAAAA,EAAUC,KACdD,EAAQE,iBAAiB,UAAW,SAACC,GACjC5J,EAAG+D,KAAK,WACI6F,OAAAA,EAAEhE,KAAKiE,aACN,IAAA,OACDJ,EAAQK,YAAY,CAAED,YAAa,SACnC,MAEC,IAAA,OACGrC,GAA4C,GAA5CA,OAAOC,KAAKmC,EAAEhE,KAAKmE,KAAKC,UAAU9F,OAAa,CAC3C+F,IAAAA,EAAgBd,EACfrI,MACAmD,QACGjE,EAAGoI,OACCZ,OAAOI,OAAOgC,EAAEhE,KAAKmE,KAAKC,UACrBrH,IAAI,SAAAuH,GACMA,OAAAA,EAAQb,UAIV,GAArB9I,EAAW2D,SACX+F,EAAgB,CAACA,IAErBA,EAAgBA,EAActH,IAAI,SAAAwH,GAWvBA,OAVPA,EAAenK,EAAGoK,QAAQD,EAAc,GACxCA,EAAenK,EAAGsI,IACdtI,EAAGyD,IACC0G,EACA,EAAIA,EAAa9H,MAAM,IAE3B,KAOJsG,IAAAA,EAAU,GACV0B,EAAiBrK,EAAGsK,OAAOL,GAC1BvF,OAAO,GACP6F,QAAQ,CAACN,EAAc/F,QAAS,IAChCsG,UAAU,CAAC,EAAG,IACd3D,YAED4D,EAAsBzK,EAAGuI,YAAYvI,EAAGsK,OAAOL,GAAgB,EAAG,MAAM,GACvEM,QAAQ,CAACN,EAAc/F,QAAS,IAChCsG,UAAU,CAAC,EAAG,IACd3D,YAELW,OAAOC,KAAKmC,EAAEhE,KAAKmE,KAAKC,UACnBlD,QAAQ,SAAC4D,EAAY1D,GACdlE,KAAK6C,SAAWiE,EAAEhE,KAAKmE,KAAKC,SAASU,GAAYC,wBACjDhC,EAAQ+B,GAAcD,EAAoBzD,GAE1C2B,EAAQ+B,GAAcL,EAAerD,KAIjDQ,OAAOC,KAAK2B,GAAatC,QAAQ,SAAC4D,QAC8CjC,IAAxEjB,OAAOC,KAAKmC,EAAEhE,KAAKmE,KAAKC,UAAUY,KAAK,SAAAC,GAAQA,OAAAA,IAASH,KACjB,GAAnCtB,EAAYsB,GAAYpB,SACxBH,EAAW2B,MACP1B,EAAYsB,GAAYrB,MACxBO,EAAEhE,KAAKmE,KAAKC,SAASU,GAAY/B,QACjCiB,EAAEhE,KAAKmE,KAAKC,SAASU,GAAY9B,QACjCgB,EAAEhE,KAAKmE,KAAKC,SAASU,GAAYrB,OAGzCD,EAAYsB,GAAYpB,SAAU,GAElCF,EAAYsB,GAAYpB,SAAU,IAI1C9B,OAAOC,KAAKmC,EAAEhE,KAAKmE,KAAKC,UAAUlD,QAAQ,SAAC4D,EAAY1D,GACnDoC,EAAYsB,GAAYrB,MAAQO,EAAEhE,KAAKmE,KAAKC,SAASU,GAAYrB,QAErEI,EAAQK,YAAY,CAChBD,YAAa,OACbE,KAAM,CACFC,SAAUxC,OAAOC,KAAKmC,EAAEhE,KAAKmE,KAAKC,UAAUzH,OAAO,SAACuF,EAAK4C,GAK9C5C,OAJPA,EAAI4C,GAAc,CACd/B,QAASA,EAAQ+B,GACjBK,OAAQnB,EAAEhE,KAAKmE,KAAKC,SAASU,GAAYK,QAEtCjD,GACR,YAIX2B,EAAQK,YAAY,CAChBD,YAAa,OACbE,KAAM,CACFG,QAAS,MAKrB,MAEC,IAAA,QACDf,EAAW7H,MAAMsI,EAAEhE,KAAKmE,KAAKiB,IAAKpB,EAAEhE,KAAKmE,KAAK9E,YAAa2E,EAAEhE,KAAKmE,KAAKhF,sBACvE0E,EAAQK,YAAY,CAAED,YAAa,UACnC,MAEC,IAAA,OACD7J,EAAG+D,KAAK,WACAkH,IACAC,EADK/B,EAAWrI,MAAMK,aACXoB,OAAO,SAACuF,EAAKqD,GAEjBrD,OADPA,EAAIqD,EAAEN,MAAQM,EACPrD,GACR,IACH2B,EAAQK,YAAY,CAChBD,YAAa,OACbE,KAAM,CACFqB,cAAerL,EAAKsL,GAAGC,KAAKJ,QAKxC,MAEC,IAAA,OACGK,IAAAA,EAAcxL,EAAKsL,GAAGG,KAAK5B,EAAEhE,KAAKmE,KAAKqB,eAC3CjC,EAAWrI,MAAMK,aAAa2F,QAAQ,SAACqE,GACnCA,EAAEM,OAAOF,EAAYJ,EAAEN,SAE3B1B,EAAWlI,YAAYC,WACnBiI,EAAWrI,MAAMK,cAErBsI,EAAQK,YAAY,CAAED,YAAa","file":"agent.347b5559.js","sourceRoot":"..\\..\\cnnNLP_dddqn_webworker","sourcesContent":["import * as tf from \"@tensorflow/tfjs\"\r\nimport { registerTfex } from \"../../../lib/tfjs-extensions/src\"\r\nconst tfex = registerTfex(tf)\r\n\r\nexport class DDDQN {\r\n    constructor({\r\n        sequenceLen = 60,\r\n        stateVectorLen = 10,\r\n        embInner = [32, 32, 32],\r\n        layerNum = 8,\r\n        outputInner = [32, 32],\r\n        actionsNum = [3, 3, 4],\r\n        memorySize = 1000,\r\n        updateTargetStep = 0.05,\r\n        initLearningRate = 1e-3,\r\n        minLearningRate = 1e-5,\r\n        discount = 0.99\r\n    }) {\r\n\r\n        {\r\n            this.updateTargetStep = updateTargetStep\r\n\r\n            this.discount = discount\r\n\r\n            this.count = 0\r\n\r\n            this.actionsNum = actionsNum\r\n        }\r\n\r\n        {\r\n            this.model = this.buildModel({\r\n                sequenceLen: sequenceLen,\r\n                stateVectorLen: stateVectorLen,\r\n                embInner: embInner,\r\n                layerNum: layerNum,\r\n                outputInner: outputInner,\r\n                actionsNum: actionsNum\r\n            })\r\n            this.model.summary()\r\n\r\n            this.targetModel = this.buildModel({\r\n                sequenceLen: sequenceLen,\r\n                stateVectorLen: stateVectorLen,\r\n                embInner: embInner,\r\n                layerNum: layerNum,\r\n                outputInner: outputInner,\r\n                actionsNum: actionsNum\r\n            })\r\n\r\n            this.targetModel.setWeights(this.model.getWeights())\r\n        }\r\n\r\n        {\r\n            this.memorySize = memorySize\r\n            this.memory = []\r\n        }\r\n\r\n        {\r\n            this.minLearningRate = minLearningRate\r\n            this.initLearningRate = initLearningRate\r\n            this.optimizer = tf.train.adam(this.initLearningRate)\r\n        }\r\n\r\n    }\r\n\r\n    buildModel(\r\n        {\r\n            sequenceLen,\r\n            stateVectorLen,\r\n            layerNum = 32,\r\n            actionsNum = [3, 3, 4]\r\n        }\r\n    ) {\r\n        let stateSeqNet = (inputLayer, stateVectorLen, sequenceLen) => {\r\n            stateSeqLayer = tf.layers.conv1d({\r\n                filters: stateVectorLen,\r\n                kernelSize: [1],\r\n                activation: \"selu\",\r\n                padding: \"same\"\r\n            }).apply(inputLayer)\r\n\r\n            stateSeqLayer = tf.layers.permute({\r\n                dims: [2, 1]\r\n            }).apply(stateSeqLayer)\r\n\r\n            stateSeqLayer = tf.layers.conv1d({\r\n                filters: sequenceLen,\r\n                kernelSize: [1],\r\n                activation: \"selu\",\r\n                padding: \"same\"\r\n            }).apply(stateSeqLayer)\r\n\r\n            stateSeqLayer = tf.layers.permute({\r\n                dims: [2, 1]\r\n            }).apply(stateSeqLayer)\r\n\r\n            return stateSeqLayer\r\n        }\r\n        let input = tf.input({ shape: [sequenceLen, stateVectorLen] })\r\n\r\n        let stateSeqLayer = stateSeqNet(input, stateVectorLen * 2, sequenceLen)\r\n\r\n        for (let i = 0; i < layerNum; i++) {\r\n            stateSeqLayer = stateSeqNet(stateSeqLayer, actionsNum.reduce((a, b) => a + b, 0) * 2, sequenceLen)\r\n        }\r\n\r\n        let outputs = actionsNum.map(actionNum => {\r\n            let actionSeqLayer = stateSeqNet(stateSeqLayer, actionsNum.reduce((a, b) => a + b, 0) * 2, sequenceLen)\r\n\r\n            for (let i = 0; i < Math.ceil(layerNum ** 0.5); i++) {\r\n                actionSeqLayer = stateSeqNet(actionSeqLayer, actionsNum.reduce((a, b) => a + b, 0), sequenceLen)\r\n            }\r\n\r\n            actionSeqLayer = tf.layers.permute({\r\n                dims: [2, 1]\r\n            }).apply(actionSeqLayer)\r\n\r\n\r\n            let value = actionSeqLayer\r\n            {\r\n                value = tf.layers.conv1d({\r\n                    filters: 1,\r\n                    kernelSize: [1],\r\n                    activation: \"selu\",\r\n                    padding: \"same\"\r\n                }).apply(value)\r\n\r\n                value = tf.layers.permute({\r\n                    dims: [2, 1]\r\n                }).apply(value)\r\n\r\n                value = tf.layers.conv1d({\r\n                    filters: 1,\r\n                    kernelSize: [1],\r\n                    activation: \"selu\",\r\n                    padding: \"same\"\r\n                }).apply(value)\r\n                value = tf.layers.flatten().apply(value)\r\n            }\r\n\r\n            let A = actionSeqLayer\r\n            {\r\n                A = tf.layers.conv1d({\r\n                    filters: 1,\r\n                    kernelSize: [1],\r\n                    activation: \"selu\",\r\n                    padding: \"same\"\r\n                }).apply(A)\r\n\r\n                A = tf.layers.permute({\r\n                    dims: [2, 1]\r\n                }).apply(A)\r\n\r\n                A = tf.layers.conv1d({\r\n                    filters: actionNum,\r\n                    kernelSize: [1],\r\n                    activation: \"selu\",\r\n                    padding: \"same\"\r\n                }).apply(A)\r\n                A = tf.layers.flatten().apply(A)\r\n\r\n                A = tfex.layers.lambda({\r\n                    func: (x) => {\r\n                        return tf.sub(x, tf.mean(x, 1, true))\r\n                    }\r\n                }).apply([A])\r\n            }\r\n\r\n            let Q = tf.layers.add().apply([value, A])\r\n\r\n            return Q\r\n        })\r\n\r\n        return tf.model({ inputs: [input], outputs: outputs })\r\n    }\r\n\r\n    tQandQ(batchPrevS, batchAs, batchRs, batchNextS) {\r\n        return tf.tidy(() => {\r\n            let predictions = this.model.predict(batchPrevS)\r\n            if (this.actionsNum.length == 1) {\r\n                predictions = [predictions]\r\n            }\r\n            const Qs = this.actionsNum.map((actionNum, actionType) => {\r\n                return tf.mul(\r\n                    tf.oneHot(\r\n                        batchAs[actionType],\r\n                        actionNum\r\n                    ),\r\n                    predictions[actionType]\r\n                ).sum(1)\r\n            })\r\n\r\n            let targetPredictions = this.targetModel.predict(batchNextS)\r\n            if (this.actionsNum.length == 1) {\r\n                targetPredictions = [targetPredictions]\r\n            }\r\n            const targetQs = this.actionsNum.map((actionNum, actionType) => {\r\n                const maxQ = tf.mul(\r\n                    tf.oneHot(\r\n                        tf.argMax(\r\n                            predictions[actionType],\r\n                            1\r\n                        ),\r\n                        actionNum\r\n                    ),\r\n                    targetPredictions[actionType]\r\n                ).sum(1)\r\n                const targets = batchRs[actionType].add(maxQ.mul(tf.scalar(this.discount)));\r\n                return targets;\r\n            })\r\n            return [targetQs, Qs]\r\n        })\r\n    }\r\n\r\n    train(replayNum = 100, loadIdxes = [null], usePrioritizedReplay = false) {\r\n        tf.tidy(() => {\r\n            let train_ = (replayIdxes) => {\r\n                tf.tidy(() => {\r\n                    let replayIdxes_ = replayIdxes.slice()\r\n\r\n                    let arrayPrevS = []\r\n                    let arrayAs = new Array(this.actionsNum.length).fill([])\r\n                    let arrayRs = new Array(this.actionsNum.length).fill([])\r\n                    let arrayNextS = []\r\n\r\n                    for (let i = 0; i < replayNum; i++) {\r\n                        if (replayIdxes_[i] == null || replayIdxes_[i] >= this.memory.length) {\r\n                            replayIdxes_[i] = Math.floor(Math.random() * this.memory.length);\r\n                        }\r\n                        let data = this.memory[replayIdxes_[i]]\r\n                        // console.log(data)\r\n                        arrayPrevS.push(data.prevS)\r\n                        for (let j = 0; j < this.actionsNum.length; j++) {\r\n                            arrayAs[j][i] = data.As[j]\r\n                            arrayRs[j][i] = data.Rs[j]\r\n                        }\r\n                        arrayNextS.push(data.nextS)\r\n                    }\r\n\r\n                    let batchPrevS = tf.tensor3d(arrayPrevS)\r\n                    let batchAs = arrayAs.map((arrayA) => {\r\n                        return tf.tensor1d(arrayA, 'int32')\r\n                    })\r\n                    let batchRs = arrayRs.map((arrayR) => {\r\n                        return tf.tensor1d(arrayR, 'int32')\r\n                    })\r\n                    let batchNextS = tf.tensor3d(arrayNextS)\r\n\r\n                    let grads = this.optimizer.computeGradients(\r\n                        () => {\r\n                            let [targetQs, Qs] = this.tQandQ(\r\n                                batchPrevS,\r\n                                batchAs,\r\n                                batchRs,\r\n                                batchNextS\r\n                            )\r\n                            tf.addN(\r\n                                this.actionsNum.map((actionNum, actionType) => {\r\n                                    return tf.abs(tf.sub(targetQs[actionType], Qs[actionType]))\r\n                                })\r\n                            ).arraySync()\r\n                                .forEach((absTD, idx) => {\r\n                                    this.memory[replayIdxes_[idx]].p = absTD\r\n                                })\r\n                            let loss = tf.mean(\r\n                                tf.stack(\r\n                                    this.actionsNum.map((actionNum, actionType) => {\r\n                                        return tf.losses.huberLoss(targetQs[actionType], Qs[actionType])\r\n                                    })\r\n                                )\r\n                            )\r\n                            loss.print()\r\n                            return loss\r\n                        }, this.model.getWeights(true)).grads\r\n\r\n                    let gradsName = Object.keys(grads)\r\n                    grads = tfex.funcs.clipByGlobalNorm(Object.values(grads), 0.05)[0]\r\n\r\n                    this.optimizer.applyGradients(gradsName.reduce((acc, gn, idx) => {\r\n                        acc[gn] = grads[idx]\r\n                        // if (gn == \"weighted_average_WeightedAverage1/w\") {\r\n                        //     acc[gn].print()\r\n                        // }\r\n                        return acc\r\n                    }, {}))\r\n\r\n                    this.count++\r\n\r\n                    this.optimizer.learningRate = Math.max(this.initLearningRate / (this.count ** 0.5), this.minLearningRate)\r\n\r\n                    this.targetModel.setWeights(\r\n                        this.targetModel.getWeights().map((weight, idx) => {\r\n                            return tf.add(\r\n                                tf.mul(this.model.getWeights()[idx], this.updateTargetStep),\r\n                                tf.mul(weight, 1 - this.updateTargetStep),\r\n                            )\r\n                        })\r\n                    )\r\n                })\r\n            }\r\n            if (this.memory.length != 0) {\r\n                if (usePrioritizedReplay) {\r\n                    let prioritizedReplayBuffer = tf.tidy(() => {\r\n                        let prioritys = tf.tensor(this.memory.map(mem => mem.p))\r\n                        prioritys = tf.div(prioritys, tf.sum(prioritys, 0, true))\r\n                        // prioritys.print()\r\n                        return tf.multinomial(prioritys, replayNum, null, true).arraySync()\r\n                    })\r\n                    // console.log(prioritizedReplayBuffer)\r\n                    train_(prioritizedReplayBuffer.map((prioritizedReplayIdx, idx) => {\r\n                        return loadIdxes[idx] == null || loadIdxes[idx] == undefined ? prioritizedReplayIdx : loadIdxes[idx]\r\n                    }))\r\n                } else {\r\n                    train_(loadIdxes)\r\n                }\r\n            }\r\n        })\r\n    }\r\n\r\n    store(preState, actions, rewards, nextState) {\r\n        if (this.memory.length == this.memorySize) {\r\n            this.memory.pop()\r\n        }\r\n        this.memory.unshift({\r\n            prevS: preState,\r\n            As: actions,\r\n            Rs: rewards,\r\n            nextS: nextState,\r\n            p: 1e+9\r\n        })\r\n    }\r\n\r\n    load(index) {\r\n        if (index == null || index >= this.memory.length) {\r\n            index = Math.floor(Math.random() * this.memory.length);\r\n        }\r\n        return this.memory[index]\r\n    }\r\n\r\n}\r\n\r\nexport function dddqn({\r\n    sequenceLen = 60,\r\n    stateVectorLen = 10,\r\n    embInner = [32, 32, 32],\r\n    layerNum = 8,\r\n    outputInner = [32, 32],\r\n    actionsNum = [3, 3, 4],\r\n    memorySize = 1000,\r\n    updateTargetStep = 0.05,\r\n    minLearningRate = 1e-3,\r\n    discount = 0.99\r\n}) {\r\n    return new DDDQN({\r\n        sequenceLen,\r\n        stateVectorLen,\r\n        embInner,\r\n        layerNum,\r\n        outputInner,\r\n        actionsNum,\r\n        memorySize,\r\n        updateTargetStep,\r\n        minLearningRate,\r\n        discount\r\n    })\r\n}","export * from './model'","import * as tf from \"@tensorflow/tfjs\"\r\nimport { dddqn } from \"../../src/js/MirageNet/dddqn\"\r\nimport { registerTfex } from \"../../src/lib/tfjs-extensions/src\"\r\nconst tfex = registerTfex(tf)\r\n\r\ntf.setBackend(\"webgl\")\r\n\r\nlet actionsNum = [2, 2, 2, 2, 2, 2, 2]\r\n\r\nlet dddqnModel = dddqn({\r\n    sequenceLen: 16,\r\n    stateVectorLen: 55,\r\n    layerNum: 32,\r\n    actionsNum: actionsNum,\r\n    memorySize: 6400,\r\n    minLearningRate: 1e-4,\r\n    initLearningRate: 1e-3,\r\n    updateTargetStep: 0.1,\r\n    discount: 0.9\r\n})\r\n\r\nlet preArchives = {\r\n    \"player1\": {\r\n        state: null,\r\n        expired: true\r\n    },\r\n    \"player2\": {\r\n        state: null,\r\n        expired: true\r\n    }\r\n}\r\n\r\ntf.ready().then(() => {\r\n    let channel = self\r\n    channel.addEventListener(\"message\", (e) => {\r\n        tf.tidy(() => {\r\n            switch (e.data.instruction) {\r\n                case 'init': {\r\n                    channel.postMessage({ instruction: \"init\" })\r\n                    break\r\n                }\r\n                case 'ctrl': {\r\n                    if (Object.keys(e.data.args.archives).length != 0) {\r\n                        let outputActions = dddqnModel\r\n                            .model\r\n                            .predict(\r\n                                tf.tensor(\r\n                                    Object.values(e.data.args.archives)\r\n                                        .map(archive => {\r\n                                            return archive.state\r\n                                        })\r\n                                )\r\n                            )\r\n                        if (actionsNum.length == 1) {\r\n                            outputActions = [outputActions]\r\n                        }\r\n                        outputActions = outputActions.map(outputAction => {\r\n                            outputAction = tf.softmax(outputAction, 1)\r\n                            outputAction = tf.div(\r\n                                tf.add(\r\n                                    outputAction,\r\n                                    1 / outputAction.shape[1]\r\n                                ),\r\n                                2\r\n                            )\r\n                            // outputAction.sum(1).print()\r\n                            // outputAction.print()\r\n                            return outputAction\r\n                        })\r\n\r\n                        let actions = {}\r\n                        let chooseByArgMax = tf.concat(outputActions)\r\n                            .argMax(1)\r\n                            .reshape([outputActions.length, -1])\r\n                            .transpose([1, 0])\r\n                            .arraySync()\r\n\r\n                        let chooseByMultinomial = tf.multinomial(tf.concat(outputActions), 1, null, true)\r\n                            .reshape([outputActions.length, -1])\r\n                            .transpose([1, 0])\r\n                            .arraySync()\r\n\r\n                        Object.keys(e.data.args.archives)\r\n                            .forEach((playerName, idx) => {\r\n                                if (Math.random() < e.data.args.archives[playerName].chooseActionRandomValue) {\r\n                                    actions[playerName] = chooseByMultinomial[idx]\r\n                                } else {\r\n                                    actions[playerName] = chooseByArgMax[idx]\r\n                                }\r\n                            })\r\n\r\n                        Object.keys(preArchives).forEach((playerName) => {\r\n                            if (Object.keys(e.data.args.archives).find(name => name === playerName) !== undefined) {\r\n                                if (preArchives[playerName].expired == false) {\r\n                                    dddqnModel.store(\r\n                                        preArchives[playerName].state,\r\n                                        e.data.args.archives[playerName].actions,\r\n                                        e.data.args.archives[playerName].rewards,\r\n                                        e.data.args.archives[playerName].state,\r\n                                    )\r\n                                }\r\n                                preArchives[playerName].expired = false\r\n                            } else {\r\n                                preArchives[playerName].expired = true\r\n                            }\r\n                        })\r\n\r\n                        Object.keys(e.data.args.archives).forEach((playerName, idx) => {\r\n                            preArchives[playerName].state = e.data.args.archives[playerName].state\r\n                        })\r\n                        channel.postMessage({\r\n                            instruction: \"ctrl\",\r\n                            args: {\r\n                                archives: Object.keys(e.data.args.archives).reduce((acc, playerName) => {\r\n                                    acc[playerName] = {\r\n                                        actions: actions[playerName],\r\n                                        aiCtrl: e.data.args.archives[playerName].aiCtrl\r\n                                    }\r\n                                    return acc\r\n                                }, {})\r\n                            }\r\n                        })\r\n                    } else {\r\n                        channel.postMessage({\r\n                            instruction: \"ctrl\",\r\n                            args: {\r\n                                archive: {}\r\n                            }\r\n                        })\r\n                    }\r\n                    // console.log(\"ctrl\")\r\n                    break\r\n                }\r\n                case 'train': {\r\n                    dddqnModel.train(e.data.args.bsz, e.data.args.replayIdxes, e.data.args.usePrioritizedReplay)\r\n                    channel.postMessage({ instruction: \"train\" })\r\n                    break\r\n                }\r\n                case 'save': {\r\n                    tf.tidy(() => {\r\n                        let Ws = dddqnModel.model.getWeights()\r\n                        let tList = Ws.reduce((acc, w) => {\r\n                            acc[w.name] = w\r\n                            return acc\r\n                        }, {})\r\n                        channel.postMessage({\r\n                            instruction: \"save\",\r\n                            args: {\r\n                                weightsBuffer: tfex.sl.save(tList)\r\n                            }\r\n                        })\r\n                    })\r\n\r\n                    break\r\n                }\r\n                case 'load': {\r\n                    let loadWeights = tfex.sl.load(e.data.args.weightsBuffer)\r\n                    dddqnModel.model.getWeights().forEach((w) => {\r\n                        w.assign(loadWeights[w.name])\r\n                    })\r\n                    dddqnModel.targetModel.setWeights(\r\n                        dddqnModel.model.getWeights()\r\n                    )\r\n                    channel.postMessage({ instruction: \"load\" })\r\n                    break\r\n                }\r\n            }\r\n        })\r\n    })\r\n})"]}